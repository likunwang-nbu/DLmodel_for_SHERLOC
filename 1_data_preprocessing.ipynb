{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52eede4e-798c-40fb-aef9-ae17b0bd08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import rampy as rp\n",
    "from utils.PCA_tool import PCA_transform\n",
    "from joblib import dump\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from aggmap import AggMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97d330f-e09a-48c8-a425-f5db1cec20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rrs_csv_files(directory):\n",
    "    rrs = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            if f.endswith('.csv') and 'rrs' in f:\n",
    "                rrs.append(f)\n",
    "    return rrs\n",
    "\n",
    "def extract_data(ID, file_index, rrs_files, folder_path):\n",
    "    csv_path = os.path.join(folder_path, rrs_files[file_index])\n",
    "    data = pd.read_csv(csv_path, skiprows=[0], header=0, low_memory=False)\n",
    "    n = len(data)\n",
    "    if n == 307:\n",
    "        rows = range(3, 103)          \n",
    "    elif n == 157:\n",
    "        rows = range(3, 53)          \n",
    "    data_tmp = data.iloc[list(rows)].copy()\n",
    "    data_tmp['ID_num'] = [f'{ID}_{i}' for i in rows]  \n",
    "    return data_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f7e744-61c7-48c7-af3a-3383a9cb6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_plan = [\n",
    "    ('/sol_00257/', [0, 1]),#Dourbes\n",
    "    ('/sol_00269/', [6, 7, 2, 3, 4, 5]),#Dourbes\n",
    "    ('/sol_00207/', [0, 1]),#Garde\n",
    "    ('/sol_00208/', [4, 5, 0, 1, 2, 3]),#Garde\n",
    "    ('/sol_00293/', [0, 1]),#Quartier\n",
    "    ('/sol_00304/', [8, 9, 6, 7, 4, 5, 2, 3]),#Quartier\n",
    "    ('/sol_00162/', [0, 3]),#Guillaumes\n",
    "    ('/sol_00161/', [3, 6]),#Guillaumes\n",
    "    ('/sol_00349/', [0, 1]),#Montpezat\n",
    "    ('/sol_00186/', [3]),#Bellegarde\n",
    "    ('/sol_00370/', [0, 1]),#Alfalfa\n",
    "    ('/sol_00747/', [0, 1, 2, 3]),#Solva\n",
    "    ('/sol_00751/', [2, 0]),#Solva\n",
    "    ('/sol_00782/', [2, 3, 0, 1]),#Solitude Lake\n",
    "    ('/sol_00789/', [0, 1]),#Ouzel Falls\n",
    "    ('/sol_00790/', [0, 2]),#Ouzel Falls\n",
    "    ('/sol_00851/', [0, 1, 2, 3]),#Lake Haiyaha\n",
    "    ('/sol_00852/', [0, 1, 3, 2]),#Lake Haiyaha\n",
    "    ('/sol_00865/', [0]),#Dragon's Egg Rock\n",
    "    ('/sol_00860/', [1, 0]),#Dragon's Egg Rock\n",
    "    ('/sol_00861/', [1, 0]),#Dragon's Egg Rock\n",
    "    ('/sol_00879/', [0, 1, 2]),#Gabletop Mountain\n",
    "    ('/sol_00894/', [1, 0]),#Thunderbolt Peak\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c25ed05-9d64-443d-8b9b-645b33ed23b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sol_00257/\n",
      "/sol_00269/\n",
      "/sol_00207/\n",
      "/sol_00208/\n",
      "/sol_00293/\n",
      "/sol_00304/\n",
      "/sol_00162/\n",
      "/sol_00161/\n",
      "/sol_00349/\n",
      "/sol_00186/\n",
      "/sol_00370/\n",
      "/sol_00747/\n",
      "/sol_00751/\n",
      "/sol_00782/\n",
      "/sol_00789/\n",
      "/sol_00790/\n",
      "/sol_00851/\n",
      "/sol_00852/\n",
      "/sol_00865/\n",
      "/sol_00860/\n",
      "/sol_00861/\n",
      "/sol_00879/\n",
      "/sol_00894/\n"
     ]
    }
   ],
   "source": [
    "data0   = pd.DataFrame()\n",
    "current_id = 1\n",
    "base_path  = 'data_SHERLOC'\n",
    "\n",
    "for subfolder, file_indices in load_plan:\n",
    "    print(subfolder)\n",
    "    folder = base_path + subfolder\n",
    "    rrs    = find_rrs_csv_files(folder)\n",
    "    for idx in file_indices:\n",
    "        part = extract_data(current_id, idx, rrs, folder)\n",
    "        data0 = pd.concat([data0, part], ignore_index=True)\n",
    "        current_id += 1\n",
    "\n",
    "folder_path = 'data_SHERLOC/sol_00257/'\n",
    "rrs_files = find_rrs_csv_files(folder_path)\n",
    "data = pd.read_csv(folder_path + rrs_files[0], skiprows=[0], header=0)\n",
    "data_columns = list(data.iloc[0, :].values) + ['ID']\n",
    "data0.columns = data_columns\n",
    "\n",
    "labels = pd.read_excel('dataset/label.xlsx', engine='openpyxl')\n",
    "labels = labels[labels['type'].notna()]\n",
    "labels = labels[~labels['num'].astype(str).str.startswith('data')]\n",
    "labels = list(labels.type)+ (data0.shape[0]-labels.shape[0])*[0]\n",
    "data0['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "559a744e-6b0c-4097-be9d-eca5078800dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data0.iloc[:, 103:479]\n",
    "lambda0 = 248.6  \n",
    "wavelengths = np.array(data.columns, dtype=float) \n",
    "raman_shift = np.round((1/lambda0 - 1/wavelengths) * 1e7, 1)\n",
    "data.columns = raman_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904635ca-3302-48ae-8f46-657af12e541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_noise_scale_linear(df, low=2500, high=3000, group_size=50, eps=1e-8):\n",
    "    cols = df.columns.astype(float).to_numpy()          \n",
    "    mask = (cols >= low) & (cols <= high)\n",
    "    sel = np.where(mask)[0]                 \n",
    "    wn_sel = cols[sel].astype(np.float32)    \n",
    "    X_all = df.to_numpy(dtype=np.float32)    \n",
    "    N = X_all.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    group_id = idx // group_size\n",
    "    tail_vals = X_all[:, sel]\n",
    "    X_design = np.vstack([wn_sel, np.ones_like(wn_sel)]).T \n",
    "    XT_X_inv = np.linalg.inv(X_design.T @ X_design)         \n",
    "    pseudo = XT_X_inv @ X_design.T                        \n",
    "    beta_all = tail_vals @ pseudo.T                        \n",
    "    a = beta_all[:, 0:1]                               \n",
    "    b = beta_all[:, 1:2]                                 \n",
    "    baseline = a * wn_sel[None, :] + b\n",
    "    noise = tail_vals - baseline\n",
    "    rmsd_i = np.sqrt((noise**2).mean(axis=1))             \n",
    "    group_noise = pd.Series(rmsd_i).groupby(group_id).median().to_dict()\n",
    "    scales = np.array([group_noise[g] for g in group_id], dtype=np.float32)\n",
    "    X_scaled = X_all / (scales[:, None] + eps)\n",
    "    df_scaled = pd.DataFrame(X_scaled, index=df.index, columns=df.columns)\n",
    "    return df_scaled, group_noise, rmsd_i\n",
    "    \n",
    "data_scaled, group_noise, rmsd_i = group_noise_scale_linear(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72064e40-f0a5-4d04-910a-d56ad8ed4c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing negatives: 100%|█████████████████████████████████████████████████████████| 4500/4500 [00:01<00:00, 3366.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_negative(data, method = 'linear'):   \n",
    "    cleaned = []\n",
    "    it = range(data.shape[0])\n",
    "    it = tqdm(it, desc = 'Removing negatives', total = data.shape[0])\n",
    "    for i in it:\n",
    "        y = data.iloc[i].to_numpy(dtype = float).reshape(-1)\n",
    "        y_nan = y.copy()\n",
    "        y_nan[y_nan < 0] = np.nan\n",
    "        y_interp = pd.Series(y_nan).interpolate(\n",
    "            method = method, limit_direction = 'both'\n",
    "        ).to_numpy()\n",
    "        cleaned.append(y_interp)\n",
    "    cleaned = pd.DataFrame(cleaned, index = data.index, columns = data.columns)\n",
    "    return cleaned\n",
    "data_positive = remove_negative(data_scaled, method = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c9e287-6d73-485d-8815-73ee8f69bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline correcting (drPLS): 100%|██████████████████████████████████████████████████| 4500/4500 [00:49<00:00, 90.80it/s]\n"
     ]
    }
   ],
   "source": [
    "def baseline_drpls(x, data):\n",
    "    x = np.asarray(x, dtype=float).reshape(-1)\n",
    "    bir = np.array([[float(x.min()), float(x.max())]], dtype = float)\n",
    "    corrected, baselines = [], []\n",
    "    for i in tqdm(range(data.shape[0]), desc = 'Baseline correcting (drPLS)'):\n",
    "        y = data.iloc[i].to_numpy(dtype = float).reshape(-1)\n",
    "        y_corr, bsl = rp.baseline( x, y, bir, method = 'drPLS')\n",
    "        corrected.append(np.asarray(y_corr).reshape(-1))\n",
    "        baselines.append(np.asarray(bsl).reshape(-1))\n",
    "    corrected = pd.DataFrame(corrected, index=data.index, columns=data.columns)\n",
    "    baselines = pd.DataFrame(baselines, index=data.index, columns=data.columns)\n",
    "    return corrected, baselines\n",
    "data_corrected, baselines = baseline_drpls(raman_shift, data_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdcb6410-6f5c-4e61-9fd9-ea81fd3d60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([data_corrected.iloc[:,:58], data0.iloc[:, -2:]], axis = 1)\n",
    "result.to_csv('dataset/data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505a61dc-f8e1-4599-b81f-e8f9dc117556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 58)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = result.iloc[:,:58]\n",
    "X.columns = X.columns.astype(str)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca16eabf-07b5-4cf2-9315-cc7a5f5556f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 55)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pca = PCA_transform(X,10)\n",
    "df_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17c95c1-bf9d-4f4c-986a-2508238f6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-01 17:31:18,339 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - Calculating distance ...\u001b[0m\n",
      "2025-12-01 17:31:18,342 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - the number of process is 16\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#############################################################################| 1653/1653 [00:00<00:00, 4415.04it/s]\n",
      "100%|##########################################################################| 1653/1653 [00:00<00:00, 1973579.42it/s]\n",
      "100%|##################################################################################| 58/58 [00:00<00:00, 735.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-01 17:31:19,157 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - applying hierarchical clustering to obtain group information ...\u001b[0m\n",
      "2025-12-01 17:31:23,336 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - Applying grid assignment of feature points, this may take several minutes(1~30 min)\u001b[0m\n",
      "2025-12-01 17:31:23,346 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - Finished\u001b[0m\n",
      "2025-12-01 17:31:23,352 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - Calculating distance ...\u001b[0m\n",
      "2025-12-01 17:31:23,353 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - the number of process is 16\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#############################################################################| 1485/1485 [00:00<00:00, 6778.72it/s]\n",
      "100%|##########################################################################| 1485/1485 [00:00<00:00, 1351093.59it/s]\n",
      "100%|##################################################################################| 55/55 [00:00<00:00, 765.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-01 17:31:24,053 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - applying hierarchical clustering to obtain group information ...\u001b[0m\n",
      "2025-12-01 17:31:24,179 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - Applying grid assignment of feature points, this may take several minutes(1~30 min)\u001b[0m\n",
      "2025-12-01 17:31:24,187 - \u001b[32mINFO\u001b[0m - [bidd-aggmap]\u001b[0m - Finished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mp_sar = AggMap(X, metric = 'euclidean')\n",
    "mp_sar = mp_sar.fit(cluster_channels = 7, n_neighbors=15, min_dist=0.1, verbose = 0)\n",
    "mp_car = AggMap(df_pca,metric = 'euclidean')\n",
    "mp_car = mp_car.fit(cluster_channels = 7, n_neighbors=15, min_dist=0.1, verbose = 0, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a7e4b48-57bd-4852-ab86-37410a82bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#############################################################################| 4500/4500 [00:00<00:00, 6323.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dataset/sherloc_x1.data']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = mp_sar.batch_transform(X.values,scale_method = 'minmax')\n",
    "dump(X1,'dataset/sherloc_x1.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4cbb86a-b685-4242-8d74-95098bf002ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#############################################################################| 4500/4500 [00:00<00:00, 6920.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dataset/sherloc_x2.data']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = mp_car.batch_transform(df_pca.values,scale_method = 'minmax')\n",
    "dump(X2,'dataset/sherloc_x2.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ac8ee-ce9d-4c20-b769-f000dd439cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aggmap",
   "language": "python",
   "name": "aggmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
